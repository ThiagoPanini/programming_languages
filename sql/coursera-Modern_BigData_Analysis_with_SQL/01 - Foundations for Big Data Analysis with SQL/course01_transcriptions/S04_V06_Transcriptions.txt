SQL is so well understood, and so widely supported that it's useful to adapt SQL, with the wide world of big data. Refitting SQL to support massive data warehouses, is a success story today because SQL gives you a concise and ambiguous way to request information from tables of any size. By the way, a data warehouse is a large analytic database emphasis on large. But there are challenges at larger scale, relational systems and especially acid compliant transactional databases based at least two major challenges with big data, transactions and data variety. Implementing acid compliant transactions with multiple concurrent users is non-trivial, even on a system with one computer. If you wanted to implement a transactional system yourself, you could start by studying the book transaction processing by Jim Gray and Andreas Reuter, over a thousand pages and go from there. In a distributed environment, the issues compound with big data, your datastore spans thousands of deaths with replicated copies of data, and computers in the cluster are connected via networking with potential irregularity in transfer times. Even worse, consider the split brain scenario. Deposes switch in your network breaks, all the computers in your cluster remain active, but because of this partial network failure, different subsets of the computers lose visibility to one another completely. This situation is also called network partitioning. It is a definite possibility in your cluster, and the system must address it in such a way that it doesn't give false results or corrupt the data by committing conflicting transactions. Implementing multistatement transactions on distributed systems is hard, and is fraught with special problems that you must take seriously if you want an acid compliance system at large scale. The other challenge for SQL on big data, is the variety of data in big data stores. SQL is easy to use with structured data, but it is almost by definition weak with unstructured and semi-structured data. The practice of schema on write is an undesirable way to handle new content for big data, while traditional RDBMSs provide great guarantees of structure in data, their inability to store a new never before seen content, hinders their ability to retain new material for new unanticipated insight. Though you have these two fundamental issues, difficulty of scaling transactions across a massive distributed data stores and schema on write being unable to store, never before seen datasets. Between these two issues, you had the reasons for a number of newer database technologies in the big data space. It's for these reasons, plus the expense of conventional database products that traditional RDBMS technologies have not scaled infinitely, and do not cover all the database needs for big data.