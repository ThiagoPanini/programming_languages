A data warehouse is created by gathering data from one, or usually multiple data stores into one place for reporting and analytics. A great strength of big data platforms today is that they allow you to manage and use data stores of much larger sizes with a lower cost per terabyte than conventional relational systems. But these big data systems give up transactions, which combine multiple insert, update, delete and select statements in a single atomic action. A number of the features in a relational system, depend on this combination of statements for their implementation, though the loss of transactions has several implications. In order to enforce uniqueness in a column, a conventional relational database system checks all rows in the column for any new row you try to add, and then permits the new row only if its value for the column does not occur in the column already. The check for existing values and the addition of a new value must be bound into a single transaction in order for this to be done correctly. Uniqueness is an important feature of a primary key. So, you can see that transactions enable a database to enforce primary key constraints on your database tables. Similarly, a database system uses transactional lookups on related tables in order to enforce foreign key constraints. Without transactions, you might have tables with unique key columns, and you might have consistent foreign key relationships between your tables, but the database technology does not guarantee this consistency in your data. You cannot assume that rows are unique or that all foreign keys are correct. It's up to you to know about this, and to keep your data organized in whatever way you require. Transactions are used by conventional relational databases to synchronize indexes with tables. Maintaining the entries in your index in lockstep with the DML you perform on your table. Without transactions, indexes will not be synchronized with your table automatically, and you would need to rebuild your indexes whenever you need them to be up to date. Database triggers also depend on transactions. Business rules and triggers and cascading DML statements and triggers both require the transactional ability to atomically combine multiple changes and queries in order to maintain database consistency. Although you could conceive of some store procedures that perform only a single action in a database or that perform multiple actions but not atomically, these would give you a radically smaller subset of the store procedures you'd want to have in order to build up application logic in your database. Though as a practical matter, effective store procedure programming also requires transactions. Without transactions, database triggers and stored procedures are not possible. Because of the difficulty of synchronization over multiple distributed copies of file data, dump file systems like HDFS, the Hadoop Distributed File System, lack the ability to update file content in place. Following from this limitation, big data warehouse systems give up SQL update, and delete statements that can change values on individual rows or delete individual rows. The workaround is to rebuild tables completely including desired changes in a batch process. As a side note, I want to mention that when you use Impala on Apache Kudu as your data store, you can manipulate individual rows with UPDATE and DELETE statements. It is this capability that makes Kudu a useful new system or a combination of operational and analytic work. But without this special feature of a storage system like Kudu, individual UPDATE and DELETE statements are not possible.