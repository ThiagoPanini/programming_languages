In a conventional RDBMS, when you issue a create table statement to create a new table in a database, the system handles all the implementation details. The system checks any foreign key constraints to make sure that they are legal. Sets up files needed to store the table, adds index files for any unique column constraints, and then records all these details about your table, in a special part of the database called the data dictionary. Now, you can proceed to insert rows in your table, and the database system stores content in all the right files, to durably store your data and then uses those files to handle your subsequent select statements reliably, and correctly. It's great that the system lets you think in SQL, without having to worry about any of the lower level details. However, whether you like it or not, the system forces you to always think in SQL. Data storage is encapsulated by your database software. Other programs cannot access the data storage directly. File access is usually blocked to any program other than the database software. Even with access, files are usually of a proprietary format that is not usable, except through the database software. The database system keeps the data dictionary, which is a set of internally maintain tables about your tables. With table names, column names, and properties, constraint definitions, and so forth. The data dictionary tables, record your table definitions. So they comprise, data about your data, or metadata. The data dictionary is tightly coupled with your tables. It is always kept in exact alignment, accurately describing the tables you create. If you drop a table, the rows regarding that table in the data dictionary tables are automatically deleted. If you alter a table, the pertinent rows in the data dictionary are updated, and so forth. Through the mechanism, other relational database software, together with the encapsulation of file storage, and the data dictionary tightly coupled to data, RDBMSs provide SQL as the only way to access data in their databases. In contrast, the data stores and big data systems, can have SQL as one way to access data, with other forms of access also available. The data store in your big data system can be called a data lake, or data reservoir, or enterprise data hub. All of these are terms you may see often. The data lake can retain large varieties of data of all sorts. Some contents may be structured, and so easily usable by a SQL engine like Hive or Impala, and some not structure. While the traditional RDBMS only supports structured data with access only through SQL, a big data system supports a variety of data, and also a variety of ways to access, and use the data. Some programs read and write content directly to the data lake, using direct file access. These can be simple programs written in languages like Python, or Java, or C or large scale distributed applications, like MapReduce, or Spark programs. These programs can access files or potentially, any format and type, and are suitable for working with structured or unstructured data. In order to use SQL on your data, you create table definitions in a metastore, which, for Hive and Impala, happens to be called the Hive Metastore, because of its origin as a part of Hive. The metastore takes the place of the data dictionary in an RDBMS. It contains table definitions that enable table like access to some of the contents in the data lake. The metastore is not kept directly in the data lake, but alongside it. Because of the way big data systems, separate your data, and metadata, you can create table definitions that are loosely coupled to files. When your data and metadata are loosely coupled, your table definitions are not necessarily in lockstep with all your data. In fact, some files may reside in the data lake without any information about them in the metastore at all. The table definitions also do not govern the file contents, but instead provide schema on read to let you view the files in table form. This lesser data lake accept files of any sort, and you can still analyze your file contents with the SQL engine like Impala or Hive, using SQL statements like those familiar to so many other systems. Impala and Hive share the one metastore, to find table and column definitions, and then access files in the data lake on your behalf, when you issue SQL statements. Other applications like Spark programs, can optionally consult the metastore to find out about table definitions for data, but this is not required in order to access the data. A single file may be used by an Impala query, a Hive query, a general purpose Spark program, or any number of other programs. This is especially true, when the file has structured, or semi-structured contents.