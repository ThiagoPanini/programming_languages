A feature that was added to SQL for querying bigger tables, is table partitions. This feature is actually not limited to big data systems. Table partitioning is a well-established feature of virtually every RDBMS that seeks to aid performance of queries as tables grow larger. By the way, writers and speakers often use the word partitioning alone to refer to either table partitioning or network partitioning. These are completely different concepts and you need to pay attention to context to know which of these is being discussed. The idea of table partitioning is simple. You take a bigger table and the system lets you specify some simple logic, the store's rows in physically separate file directories or partitions according to some value found in the rows. For example, a table of worldwide retail sales can store rows in a separate partition for each country. Now if you run a select statement to find say, the total sales figures in the year 2015 for Australia, the system would disregard all rows except those found in the partition for country Australia. This quick dismissal of unneeded partitions from your query is called partition pruning. It means that the system needs to further process only a fraction of the data and the query result is assured to be nevertheless correct. This is especially helpful for query performance on large tables. Notice that partitioning on country does not help your query at all if the query does not include country in the qualifications for data you want to analyze. So, whether and how to use partitioning is a matter for you to decide based on the queries you plan to run most often on your large tables. Table bucketing is similar to table partitioning. It subdivides the rows in your large table into separate areas of storage but usually in a random or pseudorandom way rather than a straightforward predictable way. Bucketing can help performance when you want an analysis based on an arbitrary sample of your table and not all the data. Big data stores also support a variety of file formats, re-structured and semi-structured data. One simple file format is a CSV file, where records are stored as text and fields are separated by commas. Fields can be delimited by tabs or some other character as well. Other text-based format supported include XML and JSON. A completely different format developed for use with big data is defined by the Apache Avro project. Avro defines a binary file format for saving structured data to disk. Binary files are not character-based and so there's no way to show you a useful example here. Avro files require far less storage space than a text-based format especially when you have lots of numeric values in your rows. But there isn't just one binary file format. Another format is Apache Parquet. Parquet files like Avro files, store structured records in a binary format but it's a different format from Avro and it can be even more space efficient than Avro for many types of tables. Files can be compressed for even more space savings and encrypted for data security if needed. Your big data store can have different files and any mix of these formats and any other formats you can find useful or that may be developed in the future. So, you have no lock into one vendor or one kind of file. Another major addition to SQL, is a set of additional data types called complex data types. One principle of normalized table design in most relational databases is to keep all your column types atomic. In other words, each field of a row should store exactly one thing. Here is a partial list of the datatype supported in Impala. The datatypes in hive are similar with a few more on the list. The thing to note about these data types is that all of them with the exception of the character types, force you into using an atomic value in each part of a row. The character types allow lists of words or unstructured text but that is an exception and good design dictates that you should be aware of this and plan your tables carefully, whether you keep your character columns atomic or not. The additional complex types in Impala, also supported in hive are these. The array data type let's you put multiple values of one tie into a column. For example, with the array type, the movie table might look like this. In this example, the actor's column is an array of names and the show time's column is an array of time. A normalized design for this data might look like this. In these normalized tables, the primary key for actor and movie and movie show time is a composite containing both columns of the table. The table design with the array columns is a deliberately denormalized design. In fact array columns are examples of repeating groups in the design and with repeating groups, the title is not even in first normal form. A map column is also a repeating group design except that each item in the column is a key value pair. Look at this table. The map data type for the phone's column allows the table to conveniently store any number of keyed phone numbers for one customer in a single row. One potential benefit of these complex data types is that they allow your selects to easily fetch all the contents from one table at runtime rather than having to compute results from multiple tables and this can result in improved query time. The struct data type, also stores multiple values in one column and you will learn more about its use with hands-on practice in a later course in this specialization. I've introduced only the core basic statements of SQL throughout this course. There are a number of finer details that are specific to the SQL dialects used by Hive and Impala and it is like that with any SQL dialect. You will learn these details as you practice Hive SQL and Impala SQL in the other courses of this specialization.